########################################################
Purpose:
This script parses raw EA (Evolutionary Algorithm) run logs from the SPDP–CEW search, computes per-workload summaries, and identifies globally dominant templates (block scheme, basis, gadget radius, etc.).
It also computes optional correlations between CEW and SPDP rank proxies.

Features:

Reads your raw CSV (ea_summary.csv) generated by the EA runs.

Extracts genome/configuration fields automatically.

Computes per-workload best CEW / rank proxy and representative genome.

Reports dominant template components (≥60 % of workloads).

Saves both a digest (ea_summary_digest.csv) and a plain-text report (ea_findings.txt).

Usage Example:

python analyze_ea_results.py


Outputs:

/mnt/data/ea_summary_digest.csv   # compact summary per workload
/mnt/data/ea_findings.txt         # plain-text dominant template report


Typical EA Report:

Workloads analyzed: 8
Dominant template components (≥60% of workloads):
  - holo_basis: diag
  - gadget_radius: 1
  - Pi_plus_variant: A
Global correlation (CEW vs rank-proxy): 0.41

########################################################



import json
import pandas as pd
from pathlib import Path
from statistics import mode, StatisticsError
from collections import defaultdict, Counter
import numpy as np
import caas_jupyter_tools as cj

summary_path = Path("/mnt/data/ea_summary.csv")
df = pd.read_csv(summary_path)
cj.display_dataframe_to_user("EA Run Summary (raw)", df)

cols = list(df.columns)
workload_col = next((c for c in cols if c.lower() in ["workload", "task", "benchmark"]), None)
cew_col = next((c for c in cols if "cew" in c.lower()), None)
rank_col = next((c for c in cols if ("spdp" in c.lower() and "proxy" in c.lower()) or "rank_proxy" in c.lower() or c.lower()=="rank"), None)
genome_col = next((c for c in cols if "genome" in c.lower() or "config" in c.lower() or "template" in c.lower()), None)

if workload_col is None:
    workload_col = "workload"
    df[workload_col] = "unknown"

# Coerce numerics
for c in [cew_col, rank_col]:
    if c:
        df[c] = pd.to_numeric(df[c], errors="coerce")

# Parse genome JSON if present
parsed_fields = []
if genome_col:
    def parse_field(s):
        if isinstance(s, str):
            try:
                return json.loads(s)
            except Exception:
                return None
        return None
    df["_genome"] = df[genome_col].apply(parse_field)
    keys = set()
    for g in df["_genome"].dropna():
        if isinstance(g, dict):
            keys.update(g.keys())
    parsed_fields = sorted(keys)
    for k in parsed_fields:
        df[f"g_{k}"] = df["_genome"].apply(lambda d: d.get(k) if isinstance(d, dict) else None)

# Build per-workload digest using per-subgroup best masks (no reliance on global flags)
summary_rows = []
for w, sub in df.groupby(workload_col, sort=False):
    row = {"workload": w}
    if cew_col and sub[cew_col].notna().any():
        cew_min = sub[cew_col].min()
        best_mask = np.isclose(sub[cew_col], cew_min)
        row["best_cew"] = float(sub.loc[best_mask, cew_col].min())
        row["median_cew"] = float(sub[cew_col].median())
    elif rank_col and sub[rank_col].notna().any():
        rmin = sub[rank_col].min()
        best_mask = np.isclose(sub[rank_col], rmin)
        row["best_rank_proxy"] = float(sub.loc[best_mask, rank_col].min())
        row["median_rank_proxy"] = float(sub[rank_col].median())
    else:
        best_mask = sub.index == sub.index.min()
    # template modes among best
    template_summary = {}
    for k in parsed_fields:
        colk = f"g_{k}"
        if colk in sub.columns:
            vals = [v for v in sub.loc[best_mask, colk].tolist() if pd.notna(v)]
            if vals:
                try:
                    template_summary[k] = mode(vals)
                except StatisticsError:
                    c = Counter(map(str, vals))
                    template_summary[k] = c.most_common(1)[0][0]
    row["template_mode"] = json.dumps(template_summary) if template_summary else "{}"
    # Also keep a representative best genome for inspection
    if genome_col:
        best_genomes = sub.loc[best_mask, genome_col].dropna().astype(str).tolist()
        row["best_genome_example"] = best_genomes[0] if best_genomes else ""
    summary_rows.append(row)

summary = pd.DataFrame(summary_rows)
out_summary_path = Path("/mnt/data/ea_summary_digest.csv")
summary.to_csv(out_summary_path, index=False)
cj.display_dataframe_to_user("EA Summary (per-workload digest)", summary)

# Dominant template components across workloads
dominant = {}
if not summary.empty and "template_mode" in summary.columns:
    counters = defaultdict(Counter)
    for _, r in summary.iterrows():
        try:
            tm = json.loads(r["template_mode"]) if isinstance(r["template_mode"], str) else {}
        except Exception:
            tm = {}
        for k, v in tm.items():
            counters[k][str(v)] += 1
    W = max(1, summary.shape[0])
    for k, cnt in counters.items():
        v, n = cnt.most_common(1)[0]
        if n / W >= 0.6:
            dominant[k] = v

# Correlation CEW vs rank proxy if both present
corr = None
if cew_col and rank_col and df[cew_col].notna().sum() > 3 and df[rank_col].notna().sum() > 3:
    corr = df[[cew_col, rank_col]].corr().iloc[0,1]

report_lines = [f"Workloads analyzed: {summary.shape[0]}"]
if dominant:
    report_lines.append("Dominant template components (≥60% of workloads):")
    for k,v in dominant.items():
        report_lines.append(f"  - {k}: {v}")
else:
    report_lines.append("No single template component dominated ≥60% of workloads.")
if corr is not None and not np.isnan(corr):
    report_lines.append(f"Global correlation (CEW vs rank-proxy): {corr:.3f}")
report_path = Path("/mnt/data/ea_findings.txt")
report_path.write_text("\n".join(report_lines))

{"digest_path": str(out_summary_path), "report_path": str(report_path), "dominant": dominant, "corr": None if corr is None or np.isnan(corr) else float(corr)}
